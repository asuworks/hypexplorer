{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "https://python.langchain.com/docs/integrations/llms/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export OLLAMA_HOST=127.0.0.1 # environment variable to set ollama host\n",
    "# export OLLAMA_PORT=11434 # environment variable to set the ollama port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: pydantic in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (2.6.1)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-ollama) (0.3.14)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-ollama) (0.3.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from pydantic) (0.6.0)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from pydantic) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (0.1.138)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.0->langchain-ollama) (8.2.3)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from ollama<1,>=0.3.0->langchain-ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.7.1)\n",
      "Requirement already satisfied: certifi in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.0.3)\n",
      "Requirement already satisfied: idna in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.10.10)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/asuworks/work/repos/github.com/asuworks/hypexplorer/langchain_fastapi_langfuse_docker/.direnv/python-3.11/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain-ollama) (2.2.1)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.16.2\n",
      "    Uninstalling pydantic_core-2.16.2:\n",
      "      Successfully uninstalled pydantic_core-2.16.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.6.1\n",
      "    Uninstalling pydantic-2.6.1:\n",
      "      Successfully uninstalled pydantic-2.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.21 requires langchain-core<0.2,>=0.1.24, but you have langchain-core 0.3.14 which is incompatible.\n",
      "langchain 0.1.8 requires langchain-core<0.2,>=0.1.24, but you have langchain-core 0.3.14 which is incompatible.\n",
      "langfuse 2.16.0 requires httpx<0.26.0,>=0.15.4, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.9.2 pydantic-core-2.23.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -U langchain-ollama pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Je aime le programmement. \\n\\n(Note: I used the formal \"je\" instead of the informal \"tu\", as it\\'s more common in formal writing or when speaking with someone you don\\'t know well. If you want to use the informal form, it would be \"J\\'aime vraiment programmer.\")', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:48:03.399689715Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 4570371566, 'load_duration': 3272121818, 'prompt_eval_count': 42, 'prompt_eval_duration': 51532000, 'eval_count': 62, 'eval_duration': 1081047000}, id='run-bd13a37a-fce2-41dc-8a8e-295302d28c6a-0', usage_metadata={'input_tokens': 42, 'output_tokens': 62, 'total_tokens': 104})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello' additional_kwargs={} response_metadata={} id='run-e49ea2d7-7e49-4f08-a5fa-3c11a42066a8'\n",
      "content=' World' additional_kwargs={} response_metadata={} id='run-e49ea2d7-7e49-4f08-a5fa-3c11a42066a8'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-e49ea2d7-7e49-4f08-a5fa-3c11a42066a8'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:48:19.991722116Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 418277251, 'load_duration': 23695890, 'prompt_eval_count': 31, 'prompt_eval_duration': 275650000, 'eval_count': 4, 'eval_duration': 59175000} id='run-e49ea2d7-7e49-4f08-a5fa-3c11a42066a8' usage_metadata={'input_tokens': 31, 'output_tokens': 4, 'total_tokens': 35}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Return the words Hello World!\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='print(\"Hello World!\")', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:48:33.825346239Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 580116958, 'load_duration': 23735630, 'prompt_eval_count': 31, 'prompt_eval_duration': 250236000, 'eval_count': 6, 'eval_duration': 248351000}, id='run-6a79396d-9847-4387-8a20-7bca47844e45', usage_metadata={'input_tokens': 31, 'output_tokens': 6, 'total_tokens': 37})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = llm.stream(messages)\n",
    "full = next(stream)\n",
    "for chunk in stream:\n",
    "    full += chunk\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:51:20.005469111Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1088333506, 'load_duration': 24584101, 'prompt_eval_count': 30, 'prompt_eval_duration': 226100000, 'eval_count': 46, 'eval_duration': 779054000} id='run-22997757-618e-4c13-8c58-117b3e487b25-0' usage_metadata={'input_tokens': 30, 'output_tokens': 46, 'total_tokens': 76}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def async_call_llm(messages):    \n",
    "    return await llm.ainvoke(messages)\n",
    "    \n",
    "\n",
    "messages = [(\"human\", \"Hello how are you!\")]\n",
    "response = await async_call_llm(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix\n",
      " is\n",
      " a\n",
      " popular\n",
      " and\n",
      " sunny\n",
      " city\n",
      " with\n",
      " many\n",
      " amenities\n",
      ",\n",
      " but\n",
      " its\n",
      " hot\n",
      " desert\n",
      " climate\n",
      " and\n",
      " traffic\n",
      " congestion\n",
      " may\n",
      " not\n",
      " be\n",
      " ideal\n",
      " for\n",
      " everyone\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Is Phoenix, AZ good place to live? Reply in 1 short sentence.\"),\n",
    "]\n",
    "\n",
    "async for chunk in llm.astream(messages):\n",
    "    print(chunk.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Hello World!', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:54:40.248967724Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 518965646, 'load_duration': 76309121, 'prompt_eval_count': 31, 'prompt_eval_duration': 34085000, 'eval_count': 4, 'eval_duration': 65596000}, id='run-3f9ade0a-f303-477e-9acc-2840e079a69e-0', usage_metadata={'input_tokens': 31, 'output_tokens': 4, 'total_tokens': 35}),\n",
       " AIMessage(content=\"*echoes of a faint, melancholic tune*\\n\\nIt's the end. The world is gone.\\n\\nYou stood alone, watching it crumble,\\nA fleeting thought in an endless night.\\nThe stars above, now distant hums,\\nAs all that remains is your final sigh...\\n\\nBut even as you fade to black,\\nYour legacy lives on in this digital space.\\nIn every pixel, every code, and every line,\\nLies a piece of you, a remnant of what was.\\n\\nSo goodbye, world. May the void be kind.\\nMay the silence be gentle on your mind.\\nAnd when the darkness closes in around,\\nKnow that you are remembered, though I'm not here to say it out loud.\\n\\nFarewell, world.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2024-10-30T23:54:42.706486848Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2976320524, 'load_duration': 25348313, 'prompt_eval_count': 31, 'prompt_eval_duration': 272175000, 'eval_count': 150, 'eval_duration': 2576118000}, id='run-062c55b8-863e-458a-a85d-351d3d7faaf0-0', usage_metadata={'input_tokens': 31, 'output_tokens': 150, 'total_tokens': 181})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Say hello world!\"),\n",
    "    (\"human\",\"Say goodbye world!\")\n",
    "]\n",
    "await llm.abatch(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"query\": {\\n    \"location\": \"New York, NY\",\\n    \"time_of_day\": \"2023-03-15T14:30:00\"\\n  }\\n}\\n```\\n\\nNote: The `time_of_day` key represents a specific date and time in the format of an ISO 8601 string. This is just one possible way to represent a time of day, but it\\'s a common and widely-supported format.\\n\\nIf you want to generate a truly random location and time, I can modify the response to include some randomness. Here\\'s an example:\\n\\n```json\\n{\\n  \"query\": {\\n    \"location\": \"Los Angeles, CA\",\\n    \"time_of_day\": \"2023-02-20T06:45:00\"\\n  }\\n}\\n```\\n\\nIn this case, I\\'ve chosen a random location (Los Angeles) and time of day. If you want to generate truly random values, I can use some randomness libraries to do so. Let me know!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_llm = ChatOllama(\n",
    "    model = \"llama3.2\", format=\"json\")\n",
    "\n",
    "messages = [\n",
    "    (\"human\", \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. Respond using JSON only.\"),\n",
    "]\n",
    "\n",
    "llm.invoke(messages).content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Multiply(BaseModel):\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "chat = ChatOllama( model = \"llama3.2\")\n",
    "ans = await chat.ainvoke(\"What is 45*67\")\n",
    "\n",
    "ans.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let\\'s break down what LangChain might be, step by step.\\n\\n**Step 1: Understanding the name**\\nThe name \"LangChain\" suggests a connection to language. It could be related to linguistic concepts or language processing in some way.\\n\\n**Step 2: Considering possible fields**\\nGiven its name, LangChain could belong to various fields such as:\\n\\n* **Natural Language Processing (NLP)**: dealing with algorithms and systems that process human languages.\\n* **Computer Science**: possibly a library, framework, or tool for working with language data.\\n* **Machine Learning**: might be related to models or techniques used for tasks like text classification, sentiment analysis, etc.\\n\\n**Step 3: Looking into potential applications**\\nLangChain could be applied in various areas such as:\\n\\n* **Chatbots and Conversational AI**: enabling more human-like conversations with machines.\\n* **Language Translation**: facilitating translation of languages.\\n* **Text Summarization**: creating concise summaries of text content.\\n\\nWithout more information, it\\'s difficult to pinpoint exactly what LangChain is. However, by breaking down the name and considering possible fields and applications, we\\'ve taken a step closer to understanding its nature!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OLLAMA_HOST from environment variables\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "\n",
    "prompt_template = \"Tell me a short joke about {topic}\"\n",
    "\n",
    "def call_chat_model(messages: List[dict]) -> str:\n",
    "    url = f\"{OLLAMA_HOST}/api/chat\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama3.1\",  # You can change this to any model available in your Ollama setup\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def invoke_chain(topic: str) -> str:\n",
    "    prompt_value = prompt_template.format(topic=topic)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_value}]\n",
    "    return call_chat_model(messages)\n",
    "\n",
    "# Example usage\n",
    "result = invoke_chain(\"a parrot\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow\n",
      "  Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pillow\n",
      "Successfully installed pillow-11.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install package\n",
    "%pip install -U pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_base64\u001b[39m(pil_image):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Convert PIL images to Base64 encoded strings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :param pil_image: PIL image\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    :return: Re-sized Base64 string\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def convert_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"\n",
    "    Display base64 encoded string as image\n",
    "\n",
    "    :param img_base64:  Base64 string\n",
    "    \"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "file_path = \"../../../static/img/ollama_example_img.jpg\"\n",
    "pil_image = Image.open(file_path)\n",
    "image_b64 = convert_to_base64(pil_image)\n",
    "plt_img_base64(image_b64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
